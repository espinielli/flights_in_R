[["index.html", "Analysing Flights with R - UNDER CONSTRUCTION 1 How to use this book 1.1 Before you start 1.2 Ways to Use the Material 1.3 Structure of the book 1.4 What’s gone wrong?", " Analysing Flights with R - UNDER CONSTRUCTION David Marsh 2021-06-03 1 How to use this book – UNDER CONSTRUCTION! — 1.1 Before you start You need to have RStudio and R installed on your machine. These are available from IT Support as standard installations, or if you want to work on your own machine, both are open source: R download here and RStudio download here. We assume that you are familiar with air traffic data, so won’t spend time explaining the concepts. This book is available as html here and can be downloaded complete with data from here (TBD). If you’re working through the book as training, then it will help to install (TBD - training bundle) If you want a more speedy introduction, then Enrico Spinielli has covered the important steps here. Sebastien Thonnard and others have compiled an (internal) wiki on the intranet. There are also several useful ‘cheat sheets’, which we’ll introduce as we go through. Although this was written with Eurocontrol staff in mind, beyond this short section I think you’ll find everything is open to anyone. The joys of open source! 1.2 Ways to Use the Material The book aims to support you using it in a number of different ways: Training. Work through chapters in order, or pick out specific chapters if you need a refresher on a topic. Read the explanations, grab the code and run it, and test your understanding by answering questions [in square brackets in the text] and doing exercises. Reference. Use the book search (top left of this page) to find some how-to material directly. Snippets. The book builds up snippets of usable code (TBD - how complete?) for you to cut-and-paste. But it is never going to replace the huge amount of excellent information, especially in stackoverflow. A key principle is that “Google is your friend”: even if I prefer DuckDuckGo for many purposes, a google search is often more productive for R. Whether it’s for how to do something, or how to respond to an error, usually someone has already suffered, and some kind person has answered. Google it. 1.3 Structure of the book Chapters 1 and 2 should get you up and running in RStudio. Chapters 3 to 5 start with how to look at data, using a data viewer and with some initial graphs. If you’re mostly using existing code that creates simple charts, written by someone else, then these chapters should help you find your way around, and start to adapt the code for your own needs. Chapters ?? (and TBD) do more data loading and wrangling, covering the basics of loops and functions more…TBD Chapter 7 covers pulling data out of strings (parsing) using ‘regular expressions’, with NOTAMs as the application. 1.4 What’s gone wrong? Learning from mistakes is essential, so each chapter has a “what’s gone wrong?” section near the end, discussing some typical potholes that we all fall into from time to time. Look there especially if you don’t see the results from the code in the book that you were expecting. We start the book with one classic pitfall: R is case sensitive, which can be quite a culture shock if you’re brought up on other systems. This matters for filenames as well as code. When in doubt, check the case of what you’ve typed! "],["start.html", "2 Getting started 2.1 Orientation in RStudio 2.2 First project 2.3 First code 2.4 First packages 2.5 File types 2.6 What’s gone wrong? 2.7 Test yourself", " 2 Getting started We said in chapter 1 that you need first to install R and RStudio. These are separate pieces of software: R does all the statistical and graphics stuff, while RStudio provides the graphical user interface. In this chapter we get up and running in RStudio, and see some very basic R code. 2.1 Orientation in RStudio Here we take a brief look around the RStudio interface. Use ‘RStudio/Help’ to get more detailed help. The RStudio interface can be customised almost beyond recognition. We’ll use a mix of styles in the book so that you don’t get too fixed on seeing only one, but it’s probably helpful to your colleagues not to re-order the main four panes, otherwise they’ll find looking over your shoulder or screen-sharing a disorienting experience. A basic MS Windows RStudio, with work on the go, looks something like this. RStudio snapshot The main panes of the screen are: Top left: source code, shown as a number of tabs, one for each file; Bottom left: the ‘console’, which is a scratchpad for entering code, and where log output is usually shown (and some other tabs which we don’t need here); Top right: the ‘environment’ and ‘history’ tabs are of main interest. Environment is where you can explore all the data you’ve created. History is useful for re-doing something, particularly as you can search for code. Bottom right: This has several important tabs Files: for exploring files within a project, can be quicker than using the windows explorer; Plots: is where plots will appear (usually); Packages: is for checking which packages are installed, or active (see section 2.4); Help: all the details of the functions that you will need - this is usually quicker to use than googling a function (though the same help files also come up when you google, from various providers around the web). The buttons that appear around the panes are context-sensitive: they will change according to the type of file that you have open. There are some hot-keys for moving rapidly around the panes: most often, I use ctrl-1 to go to the source code, ctrl-2 for the console. You can then guess the others, or find them by trial and error. Recent versions of RStudio have a tutorial (tab in top right pane) if you need more detail. Try out the console (bottom left, ctrl-2). Try typing 3 + 4 * 2 there (and press ‘enter’). You should see “[1] 11”, meaning that the first “[1]” (and in this case only) element of your answer is 11. The spaces in that calculation are optional, but recommended for ease of reading. [Try it without the spaces.] If you’d like to see an answer with more than one element, type letters into the console. This is a built-in constant. Check the ‘help’ for ‘letters’ to see some others. 2.2 First project The console is good for quick, throw-away calculations. But it’s a bit like treating R as a pocket calculator. Instead, we want to save R code in files. While you can work with ‘bare’ files of R source code, ‘script files’ in the jargon or ‘scripts’, we think it’s tidier to use ‘projects’, for two main reasons: you can work with several shorter script files (and other types of source file that we won’t be looking at) together, which makes it easier to organise and navigate; and a project automatically remembers which directory it’s working in, so you can manage data input and output and graphic output more neatly. Create yourself a new project using ‘File/New Project’, selecting the options new project, give it the name ‘justlearning’ and browse to put it in your personal R directory. (TBD templates). The sequence should look a little like this, with variation coming from where you create the new project subdirectory. That final part depends on your system and filing habits. new project, step 2 new project, step 3 In that final step, if you are working on a Windows machine you will see a slightly different path to your ‘user directory’ under ‘Create project as a subdirectory of:’. It’s rare that you need to change this, anyway. The project will open without a code panel (which would have been top left), because you have no code yet. It looks like this. In this book we’ll assume that projects always keep data in the data directory, and save graphs to the graphs directory. You can create these quickly in your new project by copying the code from here (quick-copy icon appears top right in the code block when you move the mouse over it) and pasting it into the console (and press ‘enter’). # good to have these in every project dir.create(&quot;data&quot;) dir.create(&quot;graphs&quot;) If for some reason the directories already exist, don’t worry, you’ll just get a warning. The line beginning with ‘#’ is a comment which is ignored. You can also create these directories manually using ‘New Folder’ in the files tab (ctrl-5), but then make sure both folder names are in lower case! You could even use your operating system file explorer - these are just ordinary directories (‘folders’), there’s nothing R-special about them. When you quit RStudio it will save any data and open files in your project. So you can re-open and continue from where you left off. If you’ve opened very large files, just beware that saving a copy as you close can take some time. (see TBD) 2.3 First code Now that you’ve got a blank project, add a new blank R script file using File/New File/R Script, or the ‘file plus’ icon top left. It appears in the source code pane, top left. Immediately save it; call it ‘chapter2’ for example. The name isn’t critical here, but avoid spaces and punctuation. By default, it will be saved to the top level in the project. This is fine for many projects, though in some cases we might choose to organise code differently. It’s good practice to comment as you go along, using #. At the top of your new script put some comments saying who you are, the date, and what it is for. Then add a comment for each chunk of code, or for any lines you think might be difficult for you to understand when you come back. I forget where I read this idea, but think of comments as ‘notes for a future you’. Type this code into your new ‘chapter2’ script, either manually or copy paste. 3 + 4 # I know pi pi 1:50 cos(pi/3) # angle is in radians Unlike in the console, in a script file code doesn’t get executed as you type. You have to run it. Usually you’re either: stepping through code, running a bit at a time, in which case ctrl-enter (cmd-enter on the Mac) is easy to use: it runs the code where the cursor currently sits most of the time intelligently selecting other lines that need to run at the same time, and then moves to the next line of code, jumping over comments that are preceded by #; running all of the code, which you can do by pressing the ‘source’ button (or select all and ctrl-enter). The output appears in the console as a running log. It should look like this: 2.4 First packages The R language is expanding continually as people publish new packages for it. A ‘package’ provides a collection of functions, often some data and sometimes some new data types. If you’re starting in R, and just aiming to find your way around in and use code, then mostly what you need to know is how to load packages (and what that means), and a little about the more common ones. That’s what we cover in this section. 2.4.1 Package basics Some packages are already bundled in the basic installation of ‘R’, such as base which provides, as the name suggests, many of the most basic functions. But there are thousands of other packages, coming from: CRAN, the “Comprehensive R Archive Network” is the authoritative collection of packages. There are also various ‘mirror’ (official copy) sites hosted elsewhere, such as at Ghent University. Packages in CRAN have been through a degree of quality control, and are preferred to the less official sources. Github and other public repositories. Even Eurocontrol shares some there, such as CODA taxi times and the PRC dashboard. Home-made. Making packages is out of the scope of the book, but if you’re inside Eurocontrol, you may want to load the statfor package (see section TBD) created by Sebastien Thonnard that builds some access to Eurocontrol datawarehouses as well as nice formatting. There are two1 steps to using a package, and sometimes these get confused: Installation. You type install.packages(\"package_name\") and R finds the files for that package and saves them on your machine or on a network drive. So the files are available. You’ve done the shopping and the food is in the kitchen cupboards. Attaching or Loading. You have to make a package available for use for your session, usually with library( ). You’ve pulled the ingredients for your recipe out of the cupboard and they’re on the kitchen table. If there’s a difference between ‘attaching’ and ‘loading’ then it doesn’t matter here. A few packages are automatically loaded at the start of the session. In the packages pane, a package is listed if it is installed and ticked if it’s loaded. In the screenshot, just the base package is loaded of the ones listed in this screenshot. Clicking on the package name takes you to the documentation. There’s a manual for all functions together, as a pdf, but it’s usually easier to use the help pane to get the same material, and perhaps copy examples from the end of each help entry. More importantly here there are links to vignettes, and often now to websites with more info. Vignettes are not little stickers, but essential how-to guides mixing text and code. Often this can provide a skeleton end-to-end structure showing how to use functions, and that you can copy and adapt to your own need. You can load a package by ticking the box in the packages pane, but normally it’s done in code, as in this example. library(lubridate) # lots of date-related functions. Two packages can define a function with the same name. The CRAN repository of packages performs quality checks, but overlap between packages is not something CRAN controls. Think of CRAN as an excellent library rather than an ‘Académie française’ for R that controls which words are allowed into the language2. When there is overlap in packages, both defining a function with the same name, say, then on loading the second one ‘masks’ the first (you should see this above). Sometimes, as an alternative to loading the full package, you might see in code a ‘two colon’ usage, such as base::union. This is very common inside packages (it’s recommended), and it’s a way to insist that the first package version of the function should be used. As well as function with the same name and different results, there are often many different functions that you could use to achieve the same result, eg base::paste0() and stringr::str_c() both concatenate strings. # both concatenate two strings, inserting no separator between them base::paste0(&quot;On Ilkley Moo&quot;, &quot;r.&quot;) ## [1] &quot;On Ilkley Moor.&quot; stringr::str_c(&quot;On Ilkley Moo&quot;, &quot;r.&quot;) ## [1] &quot;On Ilkley Moor.&quot; In this example, notice that the strings to be concatenated are written in quotes. It’s recommended to use double \", rather than single ‘. And while we’re on such conventions, the space after the comma in the parameter list is optional, but recommended for ease of reading. In fact you’ll see that when there are lots of parameters or long ones, we tend to move to a new line, also for ease of reading. Meanwhile, there should be no space between the function name and the’(’. There’s an art in R to doing the most with the minimum number of packages, since it takes time to find your way around the functions in a package. That’s like the satisfaction of making the recipe from things you already have in the cupboard. But sometimes, you just don’t want to make the flaky pastry yourself. There are times when you’re looking at a complex task and should be thinking ‘surely someone has already tackled this in R?’. A little googling will often find you most of the pieces already in place in a new package, or one you have but had forgotten about. You develop a personal ‘dialect’ of R, from the packages that you choose to use most often. We’ll discuss one of the most common dialects, the tidyverse, in more detail in the next section. Since it helps if you and colleagues share a dialect, everyone adopting the tidyverse is a good start. 2.4.2 Tidyverse The tidyverse is our chosen dialect, in the sense that in most cases we’ll use the functions and data structures, and way of organising, that go with this collection of packages. There’s a lot of excellent documentation already available, so we will explain some basics here, and introduce other elements as we need them for flight data examples. The tidy in ‘tidyverse’ refers to a tidy data structure: a table with each variable in its own column and each observation on one row. While we often find flight data with years displayed across the table, and countries down the side, this is not ‘tidy’. Table 2.1: An untidy table. Country Flights2019 Flights2020 France 1.3 1.5 Germany 5.2 6.0 Table 2.1: A tidy table. Country Year Flights France 2019 1.3 Germany 2020 5.2 France 2019 1.5 Germany 2020 6.0 The main data structure used by the tidyverse is the dataframe, although increasingly the tidyverse prefers the ‘tibble’ tbl, which is a specific sort of dataframe. We won’t worry about the differences here. For most ‘quick pieces of code’, the easiest is to start your R script with a library(tidyverse) to load all the parts of the tidyverse. If you were writing a package, that wouldn’t be very efficient, because there’s quite a lot of it. Sometimes you’ll see individual parts of the tidyverse loaded including: ggplot2: lots of plotting functions (see next chapters) dplyr: for manipulating and processing data tidyr: for tidying data, such as pivot-table like actions, or splitting columns. And there are packages which are on the outskirts of the ‘tidyverse’ which get announced when you first load. The full list (at the time of writing) is as follows. ## [1] &quot;broom&quot; &quot;cli&quot; &quot;crayon&quot; &quot;dbplyr&quot; ## [5] &quot;dplyr&quot; &quot;dtplyr&quot; &quot;forcats&quot; &quot;googledrive&quot; ## [9] &quot;googlesheets4&quot; &quot;ggplot2&quot; &quot;haven&quot; &quot;hms&quot; ## [13] &quot;httr&quot; &quot;jsonlite&quot; &quot;lubridate&quot; &quot;magrittr&quot; ## [17] &quot;modelr&quot; &quot;pillar&quot; &quot;purrr&quot; &quot;readr&quot; ## [21] &quot;readxl&quot; &quot;reprex&quot; &quot;rlang&quot; &quot;rstudioapi&quot; ## [25] &quot;rvest&quot; &quot;stringr&quot; &quot;tibble&quot; &quot;tidyr&quot; ## [29] &quot;xml2&quot; &quot;tidyverse&quot; These include lubridate and stringr which we’ve already mentioned. We’ll see a lot more tidyverse usage in chapters TBD, when we get to grips with data wrangling. Just to whet your appetite, here we show two quick examples. Try the code out, but don’t worry if it’s a little cryptic at this stage, we will explain the parts in more detail later. Start with the untidy dataframe shown above. Use pivot_longer to make it tidy, all in one go selecting a number of columns (those that start with ‘Flights’) and extracting the ‘year’ from this, then pivoting to the tidy form. Then we show a pairing of group_by and summarise to produce some annual totals. This example doesn’t save its result but prints it immediately to the log. untidy &lt;- data.frame(Country = c(&quot;France&quot;, &quot;Germany&quot;), Flights2019 = c(1.3, 5.2), Flights2020= c(1.5, 6.0)) # tidy the data tidier &lt;- pivot_longer(untidy, # pivot the two columns starting with &#39;Flights&#39; cols = starts_with(&quot;Flights&quot;), # put the column names in a column called &#39;year&#39; names_to= &quot;year&quot;, # ignore the &#39;Flights&#39; bit of the name, and treat as integer names_pattern = &quot;Flights(.*)&quot;, names_transform = list(year = as.integer), # put the column values in a column called &#39;flights&#39; values_to = &quot;flights&quot;) print(tidier) ## # A tibble: 4 x 3 ## Country year flights ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 France 2019 1.3 ## 2 France 2020 1.5 ## 3 Germany 2019 5.2 ## 4 Germany 2020 6 # using groups - just print the result tidier %&gt;% group_by(year) %&gt;% summarise(total_flights = sum(flights)) ## # A tibble: 2 x 2 ## year total_flights ## &lt;int&gt; &lt;dbl&gt; ## 1 2019 6.5 ## 2 2020 7.5 Don’t worry if this demo seems complicated, or to come out of nowhere - we’ll take it more slowly in Chapter TBD. But it illustrates the principle we mentioned earlier: for many operations (like tidying a table), you’re not the first to need to do this, so there’s probably a neat way to do it in R. You’ll see another short bit of tidyverse at the beginning of the next chapter. 2.5 File types You will see lots of different file types (ie file extensions), in the files pane. The main ones to remember are: .R, .Rmd: both contain R code, though .Rmd is actually ‘r markdown’ which is a mix of code and text; .RProj: contains an R ‘project’ - if you see one of these in the directory, this is the one to open - everything else works from there (see TBD); .rda, .RDS: are different types of data file. R works easily also with .csv and .xls(x) see TBD. 2.6 What’s gone wrong? If you see a window like this, then you have started R rather than RStudio. That is a GUI and you can use it to execute R code, but you’ll find RStudio easier for all but the quickest snippets of code. If it keeps on happening you will find it helpful to associate R files (.R, .RMD and others) with RStudio rather than R. If you get errors saying a certain package is only compatible with version xx and higher (of R), and you think you’ve recently updated, are you sure you updated R rather than RStudio? (Even if this is a task for IT, perhaps they misunderstood and updated the wrong one?) The RStudio version is found from the top menu ‘RStudio/About R Studio’, the R version is seen when you first start up (see the image just above), or is printed if you type version in the console. If you’re searching in ‘help’ and a function isn’t appearing, for example in the drop-down as you type, it is probably because it comes from a package that isn’t loaded. You can finish typing, and the system will search and may find it, but also some other less good matches. Or if you know the package name, you can type ?ggplot2::geom_line for example, in the console to go straight there. 2.7 Test yourself 2.7.1 Questions Where will you (usually) find the help on functions? Which of these provides a graphical user interface (GUI): R, RStudio? Which ctrl-key combination takes you to the environment pane? What does an .rda file contain? What’s the difference between typing into a script file and into the console? What is a ‘vignette’? (teaser) What does 3 + 1:3 give? 2.7.2 Answers Bottom right. Both, though you’ll nearly always want to use RStudio. CTRL-8 One or more R datasets With the console, code is executed as each line is completed. An extended entry in the documentation showing how to put the functions of the packages together. 4 5 6 actually, there’s a third, but that’s too much detail for here↩︎ Actually, CRAN does enforce some quality control but that’s more about how packages work than what packages there might be, with which functions.↩︎ "],["firstLook.html", "3 First look at data and CO2 emissions 3.1 Looking at data: CO2 Data 3.2 Extracting variables 3.3 Extracting a few values 3.4 CO2 Scatter plot 3.5 What’s gone wrong? 3.6 Test yourself", " 3 First look at data and CO2 emissions It’s hard to get far in an analysis without first looking at the data to ask questions such as: What variables are there? Do I know what they all mean? What time period does it cover? Which countries, or airports etc, are included? In this chapter, we introduce some of the ways to take a quick look at your data. We also introduce some data on CO2 emissions per European State from aviation. In this chapter, you’ll be introduced to: read_xlsx(), &lt;-, summary, str, environment pane, View(), unique, head, c(), [], ggplot() Re-open your justlearning project (File:Recent Projects, or ‘justLearning.Rproj’). Create a new script file (File:New File:R script), to copy and paste the examples into, and save it as ‘chapter3’. Although the project re-opens your datasets and scripts, it starts in a new R session. That means that you have to re-load the package(s) that you need, as in this code. So you might find it easier to have this at the start of your ‘chapter3’ script. library(tidyverse) 3.1 Looking at data: CO2 Data We use public data on national CO2 emissions from aviation available on the EUROCONTROL/AIU website. We choose this, apart from the interest in the data themselves, because it’s a small set so quick to download, and it’s already tidy (each variable in one column). To isolate this book from changes in the original file, we use a version that we’ve saved to github. # download the file to the data folder, the &#39;mode&#39; parameter is needed on Windows machines co2_url &lt;- &quot;https://github.com/david6marsh/flights_in_R/raw/main/data/CO2_emissions_by_state.xlsx&quot; # if you want the original, which might have been updated, use this instead # co2_url &lt;- &quot;https://ansperformance.eu/download/xls/CO2_emissions_by_state.xlsx&quot; download.file(co2_url, &quot;data/CO2_emissions.xlsx&quot;, mode = &quot;wb&quot;) # load from the DATA worksheet - case sensitive! aviation_co2 &lt;- readxl::read_excel(&quot;data/CO2_emissions.xlsx&quot;, sheet = &quot;DATA&quot;) We’ve already seen the function(parameter) way to call a function in the cos(pi/3) example. Now we have something &lt;- function(parameter, parameter). This is a peculiarity of R that you just need to get used to. Think of it as saying: create something in the environment (without saying what it is just yet), then fill it (&lt;-) with the results of the function(). While you might occasionally have used a function in Excel, for example, in R basically everything you do is call functions. Now that the Excel data are downloaded, in your R script comment out the line download.files(..., because we don’t need to keep downloading if you happen to re-run the code. Comment means putting a # in front of it. Pressing shift-ctrl-C (shift-cmd-C on a Mac) does the same, and also works over many lines at once. Notice that R doesn’t mind if a function is split over several lines. If there were frequent updates, maybe you would repeatedly download (so not comment out). See (TBD) for an example. [See the Excel, now in your project data folder, for disclaimer and details.] International conventions mean that CO2 emissions are measured from flights departing airports in a state. read_xlsx automatically selects the first row as variable names. One tool in the ‘explore your data’ toolbox is summary. summary(aviation_co2) ## YEAR MONTH STATE_NAME STATE_CODE ## Min. :2010 Min. : 1.00 Length:5694 Length:5694 ## 1st Qu.:2012 1st Qu.: 3.25 Class :character Class :character ## Median :2015 Median : 6.00 Mode :character Mode :character ## Mean :2015 Mean : 6.50 ## 3rd Qu.:2018 3rd Qu.: 9.00 ## Max. :2020 Max. :12.00 ## CO2_QTY_TONNES TF ## Min. : 0 Min. : 1 ## 1st Qu.: 15665 1st Qu.: 1364 ## Median : 84312 Median : 4605 ## Mean : 336396 Mean : 16376 ## 3rd Qu.: 279711 3rd Qu.: 17073 ## Max. :3541111 Max. :120473 The summary function is fairly basic, but it gives a quick feel for what’s in the data. Often more helpful with numeric than character variables, but also useful for spotting if there are missing values NA. So we can see there are no missing values here. In this dataset we have: YEAR: An integer, not a date, but read_xlsx reads this as a real number (ie potentially having decimals). MONTH: An integer, giving the month in the year. Again, this has been assumed to be real rather than integer. STATE_NAME, STATE_CODE: A long name and the 2-letter ‘country code’ derived from the ICAO 4-letter communication address, of which the less said, the better. CO2_QTY_TONNES: Total annual CO2 emissions, in (metric) tonnes. TF: Total flights. This is departing flights. Flights through a state’s airspace that do not land are not counted, nor are arrivals from outside the state. A domestic flight is counted once, as a departure. You’ll notice a few names in there which aren’t states, such as ‘Canaries’ which is counted separately from Spain. These measurements add up cleanly (this isn’t always true in flights data), so you can get the full ‘Spain’ by adding the two. There are also some States with a ’*’ next to their names, which means there’s a footnote elsewhere in the spreadsheet. We want to keep things simple, so we will use a little data wrangling to aggregate to yearly totals for each country, then save the result as an R dataset, rather than an Excel file. These are some quite common step in data wrangling: Drop some variables. Here, just select everything except than STATE_CODE. (Not is a very thin !, so easily overlooked.)3 Summarise. group_by the relevant variables, then summarise within those groups. Here we use the summarise_at variant of summarise, which allows us quickly to apply a function sum to multiple variables. Drop some rows. There are few families and business unaffected by COVID-19. Aviation is of course no exception, and the data from 2020 are really an outlier for this reason. To keep things simple here, we omit 2020, using a filter that says ‘keep only the values before 2020’. The group_by isn’t essential for the filter, but I think it’s probably faster as written here, since the grouping process has already found all of the year 2020 values. Often you want to keep your groups, but here we don’t need to, so we ungroup at the end. Then we save the data into the data subdirectory of the project. There will be more examples of this sort of data manipulation in Chapter 4, with more explanation of what’s happening. And a lot more when we turn to data wrangling in chapters ?? and 6. annual_co2 &lt;- aviation_co2 %&gt;% select(!STATE_CODE) %&gt;% group_by(YEAR, STATE_NAME) %&gt;% summarise_at(vars(CO2_QTY_TONNES, TF), sum) %&gt;% filter(YEAR &lt; 2020) %&gt;% ungroup() save(annual_co2, file = &quot;data/annual_co2.rda&quot;) There are three other important ways to explore the data. Firstly in the environment pane (top right, CTRL-8), where you can click the first line, with name on, to see a summary. You get much the same thing in the console by typing str(annual_co2) where str is for ‘structure’. You should see something like this. We can see that there are three numeric variables (num) and two character variables (chr). All five variables have the same number of observations (437). In a tibble or dataframe the columns are always the same length. The second way to explore the structure, because this is a tibble, is just to type its name in the console. [Try it] This is useful but just bear in mind that for some data structures, this might fill up your console with a lot of output. Save it for when you’re sure you’ve a tibble. You can check by typing class(my_thing) into the console to see if my_thing is a tibble (shown as tbl). The third way gives a window to explore every observation. Click on the dataset name next to the blue arrow or type View(annual_co2) in the console (sorry about the upper case ‘V’, R is like that) and you get a tabular data explorer, which allows you to sort and filter. You should see something like this. [Try out the sorting and filtering in the view window. Filter to show only the Netherlands, and sort by total flights.] 3.2 Extracting variables To answer more questions about the data there are some more tools to summarise the values that it takes. We saw summary() works for numeric values, but what about discrete ones? There are several ways to pull one variable out of your data. We’ll use the $ notation, partly because there’s a reminder of this in the environment tab. [Where is this ‘reminder’?] # pull out all values in the column state_vbl &lt;- annual_co2$STATE_NAME states &lt;- unique(state_vbl) Look in the environment pane, state_vbl is listed under ‘Values’. It’s a (column) vector, one of the simple data types in R which is why it’s listed under ‘Values’ and not under ‘Data’. ‘Data’ is for more complex data structures, such as dataframes and tibbles. You’ve just pulled a column out of annual_co2 so not surprising that it has the same number of rows as annual_co2. And the first values are all ALBANIA, or were when this book was compiled. Really we want to know how many different states there are, and which ones. unique does what it says, and we’ve saved these as states; a variable name which to me implies ‘unique states’. You can tell how many there are from the environment pane, or you could use length(states). [How many are there?] To inspect all of these values you can just type states into the console, a good way to check the spelling of some, perhaps. [Try this. Is it ‘Canaries’ or ‘Canarias’?] The order in which the elements are shown is as in the original data, there’s no re-ordering unless you ask for it. If you’ve worked with SAS PROC SQL or other languages, it might come as a relief to hear that, in R, the order of rows stays where it’s put until you say otherwise; none of this need to sort before every operation. We’ll see some ways to handle ‘top’ values later (section 4.3). So in this case, even if the states are in alphabetical order, that’s just because the original Excel file was. 3.3 Extracting a few values We’ve just seen how to pull a variable out of a tibble, as a vector. How do we extract one or more values out of the vector that we created? The states are quite a lot to show in the console. Sometimes you just need to see a quick sample, eg to check if they’re in title case, or if they’re codes or names. head() is useful for showing you the first few (6 by default). head(states) ## [1] &quot;ALBANIA&quot; &quot;ARMENIA&quot; &quot;AUSTRIA&quot; ## [4] &quot;BELGIUM&quot; &quot;BOSNIA AND HERZEGOVINA&quot; &quot;BULGARIA&quot; If you want to pull out a single value, or a few of them, again there are multiple ways to do this, but the simplest is this. We show here two ways to select with a vector of numbers: creating a consecutive sequence of numbers (1:3); and creating a vector with an arbitrary selection (c(1, 5, 10)). states[1] ## [1] &quot;ALBANIA&quot; states[3] ## [1] &quot;AUSTRIA&quot; states[1:3] ## [1] &quot;ALBANIA&quot; &quot;ARMENIA&quot; &quot;AUSTRIA&quot; states[c(1, 5, 10)] ## [1] &quot;ALBANIA&quot; &quot;BOSNIA AND HERZEGOVINA&quot; &quot;CZECHIA&quot; We’ll deal with subsetting the whole dataset, rather than just a vector extracted from it, in the next chapter. 3.4 CO2 Scatter plot It’s hard to beat a graph as a way to explore data. So we end the chapter exploring a simple graph. The tidyverse way of doing this is to use ggplot. 3.4.1 First draft In the most basic scatter plot we have the following components, joined with a +. This + is peculiar to ggplot; another lovable quirk of R. Learn it, but get used to the idea that you’ll forget and use other conjunctions by mistake at times. In the simplest code we have: ggplot: with parameters the data to use, and an ‘aesthetic’ aes; aes: gives the x and y first, and here also says choose colour based on year; geom_point: says to plot points with these data, ie a scatter plot. ggplot(annual_co2, aes(TF, CO2_QTY_TONNES, colour = YEAR)) + geom_point() Even this simple example illustrates that: a parameter to a function (here ggplot) can be another function call (here aes()); we can specify parameters by position of appearance in the list (x and y are first and second for aes) so we don’t have to name them, or we can specify a parameter by name (colour), or a mix of both (as long as the position ones come first!); geom_point takes its aesthetics by default from the ggplot statement. We’ll see later that you can add to or override this default (eg section 4.5). There’s a (very) rough correlation along a diagonal line, but it would be interesting to know which states are above the line (more CO2 per flight) and which below. And is the change gradual, or is there much variability? 3.4.2 Improve the titles Let’s at least label the axes so that someone else can see quickly what has been plotted. We can transform variables on the fly (using the rule that a parameter can be a function call, here to the function /), so let’s convert both axes to millions (1e6 means 1*10^6, so millions with less risk of getting the number of 0s wrong). The label on the legend is meaningful, but to avoid the block capitals we can change that too within the labs() statement. It’s a ‘colour’ legend (that’s what is in the aes call), so you need to refer to ‘colour’ in the labs(). ggplot(annual_co2, aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR)) + geom_point() + labs(x = &quot;Departing Flights (millions)&quot;, y = &quot;CO2 (million tonnes)&quot;, colour = &quot;Year&quot;, title = &quot;Emissions per state&quot;) 3.4.3 and with clustering by state It’s tempting to read the graph as having a number of small clusters, each with flights and CO2 increasing with time, and assume that each of these corresponds to a single state. It would be nice to use the graph to see if that’s true. There are too many countries to give each its own shape (we’ll see shapes used more effectively in section TBD), but we can easily add a line to join the points for each state. [Working from geom_point, how do you think you would add a line?] We need both to add a line, which follows the pattern of geom_point, and group by state. That’s done in the same way that we coloured by year, in the aesthetics. There are several ways to plot a line. The most obvious one, having seen geom_point previously, is geom_line. However, this joins the points in x-axis order. We want data order, so that’s geom_path. Out of a sense of neatness, we also add a subscript to CO2. The code bquote(~CO[2]~\" (million tonnes)\") took some googling and is cryptic, but it works! ggplot(annual_co2, aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point() + geom_path() + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, title = &quot;Emissions per state&quot;) The graph isn’t ready for a presentation yet, but a story is already emerging. The lines do a pretty good job of grouping the years for each state together. We see a graph with 6 busy states, 3 of which are fairly linear, so a relatively fixed CO2/flight. Three others are more variable from year to year. Then there are two states, with flights in the 0.25-0.5 million flights range and that diverge from the main trend line, respectively higher and lower. The remaining, smaller states rather overlap in this graph. We’ll find out how to pick out and label states in the next chapter. 3.5 What’s gone wrong? Why do I get a message saying something like “could not find function …”? When you re-open RStudio, remember that it goes some of the way to restarting where you left off (opening files and loading datasets for you), but it opens in a new R ‘session’. That means that only the base packages are available. That’s why your R scripts usually start with a bunch of library() statements - you have to start by running these to get the packages back for this session. It pays to looks closely at the graph and try to explain what you see. In fact, it was only once I’d tidied the colours up that I noticed that they were not in order along the line. I had used geom_line (join in x-axis order) in place of geom_path (join in data order). So story-telling can help debugging too. Also, it pays to read the help file, even if you think you know how the function works. We’ll see more about grouping in later chapters. It’s quite common that I get errors in some tidyverse data wrangling, because the dataset is grouped, and I had forgotten that. So grouping is powerful and quick, but R remembers your groups longer than you do! 3.6 Test yourself 3.6.1 Questions Use head to view the first 10 states. Using the help file for head, how would you display the last 6 states? Which state names are followed by ’*’? Print the distinct state codes to the console. Print the 3rd, 23rd and 33rd state names to the console. What does &lt;- do? In ggplot what does + do? 3.6.2 Answers head(states, 10) head is documented alongside tail: use tail(states) (In my version of the data), typing states into the console, and by eye I see 4 states with a ’*’. We’ll see other ways to do this, later. unique(aviation_co2$STATE_CODE) states[c(3, 23, 33)] It puts the results of whatever is on the right-hand side into the object on the left. It connects parts of the definition of a graph together. In fact, this step is here just to illustrate the selection step. It’s not needed, as we’ll see in the data wrangling chapters, or you could test by removing it.↩︎ "],["filter.html", "4 Filtering a dataset and refining the CO2 graph 4.1 Sequences of functions 4.2 Filtering datasets and logical tests 4.3 Selecting the busiest states 4.4 CO2 graph for the top states 4.5 Labelling the CO2 graph 4.6 What does the graph say about CO2? 4.7 What’s gone wrong? 4.8 Exercises", " 4 Filtering a dataset and refining the CO2 graph In this chapter we improve the CO2 emissions graph, en route learning how to filter observations from a dataset, to add new variables, and to use the pipe operator %&gt;%. In this chapter, you’ll be introduced to: filter(), ==, %in%, mutate(), slice_max(), %&gt;%, geom_text_repel(), scales_colour_... On re-opening your justlearning project, you should still have the annual_co2 dataset in your environment, since RStudio saves these data and reloads on restart. If not, you should find that annual_co2.rda is saved in the data folder, and you can load it using load(\"data/annual_co2.rda\"). One R speciality here is that you don’t need to assign the result with a annual_co2 &lt;- ...; in fact the single file that we saved could have contained a number of datasets, and they’re all reloaded with their original dataset names.4 If you have closed RStudio and re-opened it, then you have a new session and need to reload packages, in this case library(tidyverse). Make yourself a new R script file for ‘chapter4’ starting with this. 4.1 Sequences of functions It’s time for a bit more syntax. We saw already that you can combine functions by making one the parameter to another. Or to put it another way, wrapping one around the other. As you combine more functions, this soon becomes hard to read, and you have to rely on the editor to help you spot whether you’ve enough brackets closed at the right point. For that reason we use a different syntax, the ‘pipe’ operator %&gt;% introduced by the magrittr package and adopted by the tidyverse. If you learn only one control-key combination in RStudio, do learn shift-ctrl-M (also shift-command-M on Mac). This must be the fastest way to type ‘%&gt;%’! [Try it in your console.] With a new line after each %&gt;% and appropriate indenting (RStudio helps with that), you get code that looks like this. This says, fill a with what you get from dataset b after applying function fun1, then function fun2. # which is clearer # with a pipe? a &lt;- b %&gt;% fun1(p1) %&gt;% fun2(p2, p3) # or without? a &lt;- fun2(fun1(b, p1), p2, p3) The pipe syntax works because fun1 actually has a parameter list that starts with the dataset to which it should be applied fun1(data, p1,...). So b %&gt;% fun1(p1) is just another way of writing fun1(b, p1). Or, the other way around, you can use %&gt;% whenever you have a function whose first parameter is the dataset to be operated on. It turns out that this is true for very many of them, and the tidyverse is designed that way. [Does the code to summarise by year that you saw in section 3.1 make more sense now that you know about %&gt;%?] 4.2 Filtering datasets and logical tests As is so often the case, in R there are several of ways to select rows from a dataframe or tibble. We’ll focus on the filter(data, test) function. For this we need to know how to construct a logical test. There are three parts to this: the logic, the test functions and what’s being tested. Logic is mostly given by &amp;, ! and |5 for ‘and’, ‘not’ and ‘or’, grouped with round brackets in the normal way. The test functions are almost as you might expect: &gt;, &lt;. However, in R you need to use == not = to test for equality. I suspect this creates the most common typo in R code! Check the documentation of dplyr::filter for a more complete list of test functions: in particular look out for %in%, which we will use shortly. One really nice touch in filter() is in the third part: what’s being tested. One of the trickiest bits of learning R is knowing how, within a function, to refer to one or more variables of the dataset. In filter() you can just use the name of the variable; so no quotes needed around the name, and the code assumes it is a variable from the data parameter, so no need to use annual_co2$.... In this example, we don’t push the result into a dataset (no a &lt;-), so it gets printed out directly. annual_co2 %&gt;% filter(YEAR == 2018 &amp; STATE_NAME %in% c(&quot;CZECHIA&quot;, &quot;ALBANIA&quot;)) ## # A tibble: 2 x 4 ## YEAR STATE_NAME CO2_QTY_TONNES TF ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018 ALBANIA 143871. 12744 ## 2 2018 CZECHIA 1381672. 90679 Note that: the order of the rows is as in the original dataset, not at all influenced by the order of the naming of the states in the test YEAR and STATE_NAME in the filter function are so-called ‘bare’ strings (ie without quotes) that name variables, and are shorthand for annual_co2$YEAR etc but “CZECHIA” is a value of a variable so needs to be a string in quotes and the test is case-sensitive. [Try changing it to “Czechia”.] 4.3 Selecting the busiest states In the previous chapter we selected the first-named states with head(). Now we do something more useful: selecting the top states by flights, using slice_max(). We need to define this a bit more clearly. ‘Top’ could mean in a particular year, or over the whole period (where flights have decreased as well as increased). We choose to mean ‘top by flights in 2019’. This is partly out of habit (top in the latest year is often the meaning) and partly because we need to introduce fewer new bits of R to implement it. The code is like this. A final novelty is that we use pull() which, as its help-file says, does the same as $ (learned in the last chapter) but looks nicer in pipes. [Where can you look at top_states to check that there are 8 values?] top_states &lt;- annual_co2 %&gt;% filter(YEAR == 2019) %&gt;% # top in year 2019 slice_max(TF, n = 8) %&gt;% # top 8 pull(STATE_NAME) slice_max() is a good example of how R changes with time. New versions of packages introduce minor or major changes. Sometimes a function is superseded (left to rot), other times it may be deprecated (you have some time to switch to a new version before it’s removed). The function top_n, which you might see lying around in legacy code, has been superseded by slice_max(). [Check out the documentation of top_n for some of the reasons.] These changes mean that just updating to the latest version of the package is not always the best idea, because you might have to spend some time checking for changes. It also means that, when searching on the web for hints, snippets and answers, you need to look at the date of the answer. ggplot in particular has changed quite a bit, so answers more than 5 years old or so might not be that helpful. 4.4 CO2 graph for the top states With top_states in place we can easily plot the data for the busiest states. We update the title, and add a footnote (caption) to explain what’s going on. We could have created a new dataset, eg top_co2 &lt;- annual_co2 %&gt;% filter(STATE_NAME %in% top_states), and then used this in the ggplot. But we only plan to use this filter once, so to avoid cluttering the environment with datasets, we filter ‘on the fly’, within the ggplot statement. In this case, this is a matter of personal preference. If the datasets were a lot larger, and we intended analysing and transforming just the top states in further graphs, the decision might be different. ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% top_states), aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point() + geom_path() + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, title = &quot;Emissions for the busiest 8 states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Busiest&#39; means most flights in 2019.&quot;) One advantage of filtering on the fly is that we can change on the fly. [Change the filter to the states not in the top 8. It takes one key press. Re-run the graph. Update the titles too.] As an analyst, I really want to know which country is which. With just 8 states shown, maybe I can use symbols? At one level it’s quick to do. Just replace group = with shape =. [Try this.] ggplot complains that it doesn’t really like using more than 6 symbols, because it becomes hard for the reader to jump between legend and graph. We could roll with it and learn how to extend the palette, but is there another solution? We can separate states by colours. Again, quite quick: replace colour = YEAR, group = STATE_NAME with colour = STATE_NAME. [Try this.] The line means we can work out the ordering of the years easily, so losing the year isn’t a big issue here. But eight colours is also a lot to tell apart. Even without colour blindness, you might think they’re clear but when projected onto a screen or printed or on a tiny phone screen, perhaps not. 4.5 Labelling the CO2 graph It’s relatively easy to follow a slightly different route: adding state names directly to the graph. This is done with geom_text added to the ggplot. We only want to label the point in 2019 rather than all years, so we create a new variable which is empty except in rows for the year 2019. mutate(a = ...) is the function for adding a variable ‘a’ to the dataset6. if_else() is how we give a value for only some years. As with the filter() function, we can refer to other variables in annual_co2 without inverted commas. This time, we amend the annual_co2 dataset itself, because we want to be able to use this in several places, not just in a single graph. The syntax a &lt;- a %&gt;% ... follows the same pattern as you saw earlier, but you’re overwriting the original dataset. The geom_text() inherits all of the aesthetics from the ggplot function, so it already knows where to find x and y coordinates, and what colour to use. We still need to tell geom_text() where to find the labels. This is also an aesthetic. annual_co2 &lt;- annual_co2 %&gt;% mutate(state_label = if_else(YEAR == 2019, STATE_NAME, &quot;&quot;)) ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% top_states), aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point() + geom_path() + geom_text(aes(label = state_label)) + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, title = &quot;Emissions for the busiest 8 states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Busiest&#39; means most flights in 2019.&quot;) This is close, but not quite good enough. The text is centred on the 2019 point, and this creates some ugly overlaps. There are lots of options in geom_text() to adjust the position, and you’ll find lots of examples on the web. So we could spend time adjusting the positions. But this is a first example of the rule: ‘surely someone has already come across this problem?’. Someone has indeed spent time to come up with good ways to deconflict and position labels on graphs. The package is called ggrepel, which you might need to install with install.packages(\"ggrepel\"), and it provides a ‘drop in’ replacement for geom_text naturally called geom_text_repel. To avoid using library(\"ggrepel\") when we’re just using one function from the package on one occasion, we use the double-colon syntax in the code (see 2.4.1). The defaults for this function work pretty well in this particular case. But there are a couple of things I’d like to fix: the block capitals and the year legend. Title case would be nicer, so we convert the STATE_NAME using the stringr package function str_to_title(). stringr is already loaded as part of the tidyverse, and since most of its functions begin ‘str_’ it’s quite easy to start searching in the help pane for the right one [Try this.]. In this case state_label already exists and we overwrite it. The other thing to improve is the year scale, which shows with meaningless decimals. We use a quick-ish fix, rather than the tidiest-possible solution. Scales, whether the axes or colours, are controlled by ggplot functions starting scales_ in this case scales_colour_steps() gets us a scale that shows the individual years. This is a ‘dirty’ solution in the sense that, if the data for 2020 get included, you might need to tweak the code; but then, we’ve hard-coded 2019 in a number of places, so this is dirty elsewhere. We’ll see cleaner options later (for example in section 5.3.1). annual_co2 &lt;- annual_co2 %&gt;% mutate(state_label = if_else(YEAR == 2019, str_to_title(STATE_NAME), &quot;&quot;)) ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% top_states), aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point() + geom_path() + ggrepel::geom_text_repel(aes(label = state_label)) + scale_colour_steps(n.breaks = 8, show.limits = TRUE) + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, title = &quot;Emissions for the busiest 8 states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Busiest&#39; means most flights in 2019.&quot;) 4.6 What does the graph say about CO2? Longer-haul flights use heavier aircraft and therefore the proportion of long-haul flights in a national mix is a major influence on CO2 per flight. For example, the Netherlands has more CO2 per flight than Norway: Norway has a significant domestic (so short-range) market, which the Netherlands does not, being instead a major long-haul hub. These two states have been relatively stable. The UK also has a proportionally larger long-haul market. And a decline in its domestic market has led to quite a rapid increase in CO2 per flight in recent years. So, the graph helps to build a story: though we needed some supporting information to provide some of the explanation. It has also become clear that we’re interested in CO2 per flight. See the exercises for a graph on that more directly. 4.7 What’s gone wrong? It’s inevitable that you will type = in tests where you mean ==. Some functions have friendly messages, since this is so common. Others less so. Watch that case! We are using the function filter(), not the function Filter() which is something else entirely. if_else is a fussy version of the base function ifelse, that we use here to maximise use of tidyverse functions. If it warns you that the ‘false’ must be something, then it has your long-term interests at heart. It just means that it’s a different type to the ‘true’. Compare ifelse(1&lt;2,\"true\",NA) and if_else(1&lt;2,\"true\",NA) where NA is the code for missing. 4.8 Exercises 4.8.1 Questions Where can you look to see that top_states has 8 values? How does geom_text() know where to place the text on the graph? Adapt geom_text() to shift all labels up 2 (million tonnes!). (Hint: Check the help file.) Adapt the final graph to show the smallest 8 states instead. (Hints: What’s the most likely counterpart to slice_max? Closely-related functions are often to be found in the same help file, so checking the help for a function you know might help you find similar ones that you don’t.) Adapt the graph to show year on the x-axis and CO2/flight on the y-axis. (Hints: Mostly changing the first aes() and deleting some elements. For a pretty graph you might google how to hide the legend “ggplot hide legend”, and how to set the breaks on the x-axis.) 4.8.2 Answers The environment pane, but you need to scroll down to “values” because it’s a simple vector. geom_text() inherits all aesthetics from the opening ggplot(..., aes(...)), which can be supplemented or over-written by its own aes(). In fact you could put the label= into the first aes(). geom_text(aes(label = state_label), nudge_y = 2). If you put the nudge_y inside the aes() you get an error (‘Ignoring unknown aesthetics: nudge_y’) because it’s not something that can vary with the values of a variable (“not an aesthetic”). geom_text_repel uses call-out lines when it can’t get the text close. You could experiment with making these a less confusing colour. You might also drop the ‘millions’. small_states &lt;- annual_co2 %&gt;% filter(YEAR == 2019) %&gt;% # in year 2019 slice_min(TF, n = 8) %&gt;% # smallest 8 pull(STATE_NAME) ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% small_states), aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point() + geom_path() + ggrepel::geom_text_repel(aes(label = state_label)) + scale_colour_steps(n.breaks = 8, show.limits = TRUE) + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, title = &quot;Emissions for the least-busy states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Busiest&#39; means most flights in 2019.&quot;) A pedant might say this shouldn’t be a line chart, but here’s one possibility. We’ll see a different version of this in section (TBD) ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% top_states), aes(YEAR, CO2_QTY_TONNES/TF, colour = STATE_NAME)) + geom_path() + # I decided the points looked too heavy ggrepel::geom_text_repel(aes(label = state_label)) + theme(legend.position = &quot;none&quot;) + # turn off legend scale_x_continuous(breaks = 2010:2019, minor_breaks = NULL) + # control the breaks labs(x = &quot;Year&quot;, y = bquote(~CO[2]~&quot; (tonnes per flight)&quot;), title = &quot;Emissions for the busiest 8 states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Busiest&#39; means most flights in 2019.&quot;) If even that doesn’t work, and for some reason the rda file is not in your data folder, go back to section 3.1, load the Excel file again, and find the code to summarise over years.↩︎ that’s a vertical bar, not an ‘l’ or ‘I’↩︎ and we use lower case for the variable name, because that’s our preference, even if the imported names don’t do this↩︎ "],["sortbars.html", "5 Sorting Bars, Saving Graphs, Facets 5.1 The simple sorted bar chart - more on CO2 5.2 Saving a plot 5.3 Plotting more than one year 5.4 What’s gone wrong? 5.5 Exercises 5.6 Extended Exercises", " 5 Sorting Bars, Saving Graphs, Facets While nothing beats a well hand-crafted chart, there are times when you want to just run the code and get a quick update, as a .png say. In this chapter we see how to do a classic sorted-bar chart and to save it to a file for use elsewhere. We need slightly different methods for simple bar charts and more complex ones. In this chapter, you’ll be introduced to: geom_col(), dodge, reorder(), ggsave(), as.factor(), factor(), arrange(), facet_wrap(), select, plots pane 5.1 The simple sorted bar chart - more on CO2 A classic visualisation is the bar chart, sorted from longest to shortest. With ggplot there are a couple of ways to get a bar chart. If you want ggplot to count the rows for you, use geom_bar. Here we already have values for the length of the bar, so we use geom_col instead (for ‘column’ chart). For the simplest bar charts, there is a quick way to get the order you want. In place of state_label for the x-axis, you give reorder(state_label, CO2_QTY_TONNES), the second being the variable to sort by. If you find the bars a bit top-heavy, put a - in front of CO2_QTY_... to reverse the order. The final novelty in this graph is coord_flip(). Forty-something State names is a lot of text to cram onto the horizontal axis. So we flip the axes. You’ll need to decide if this trick works where you want to use the graph. We’ll see other ways to separate the labels on the axes in (TBD). ggplot(annual_co2 %&gt;% filter(YEAR == 2019), aes(reorder(state_label, CO2_QTY_TONNES), CO2_QTY_TONNES/1e6)) + geom_col() + labs(x = &quot;&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), title = &quot;Aviation Emissions in 2019&quot;, caption = &quot;Source: EUROCONTROL.&quot;) + coord_flip() If you were to google ‘ggplot ordered bar chart’, you might find references to ‘factors’. That becomes necessary, in place of reorder, when the charts are more complicated. We’ll look at that in section 5.3.1. 5.2 Saving a plot You might be looking at the bar chart and thinking that it’s the wrong proportions for your need (portrait rather than landscape, or vice versa) or you might be thinking the axis labels are still a bit squashed together. The proportions on your screen will depend on a number of things including the space you have allowed for the ‘Plots’ window. Now, the plots window has an export button which you could use. It allows for re-sizing, but that means you have to do the same manual intervention each time. We prefer to use ggsave() to save the most-recent plot, and at the same time set the aspect ratio. Usually it’s worth doing this before working too much on the font sizes, since you don’t really know if there’s a problem until you’ve seen the png. Finally, we use the graphs folder we created for the project. Square seems about right for this graph (the width includes the axis text); and having one of the dimensions around 15cm also seems to produce png that are good enough for reports and slides without being too big a file. ggsave(&quot;graphs/FirstSortedBars.png&quot;, width = 15, height = 15, units = &quot;cm&quot;) 5.3 Plotting more than one year I can think of four ways to plot more than one year, and there are no doubt more than that: as staggered bars, though we probably will have to work hard to make enough space; as ‘facets’, creating one sub-plot per year; as a few graphs, merged and aligned using an dedicated package like cowplot; as multiple graphs using a loop Number (3) is particularly useful for combining graphs of different variables, but it’s a bit heavy to deal with here. We’ll deal with (4) in section (TBD) when we look at loops. The first two we will demonstrate in the next sections. 5.3.1 Staggered bars, and factors We took some shortcuts in section 5.1, which will need sorting out for the staggered bars. First we need to choose a couple of years, since there certainly isn’t room for more than two. That’s a filter that we’ve seen before. Secondly, we used state_label before because it was prettier, but this only exists for 2019, so we have to go back to using STATE_NAME. It’s probably time to turn this name into title case once and for all. The separation by year is done in the aesthetic aes() as you might expect. We want the bars to be different colours by year. In this case it’s the fill that we specify; colour would add an outline to the bars. To get the bars side by side we set the position = \"dodge\" parameter in geom_col(). The least obvious, final step is that ‘year’ needs to be a discrete variable, whereas currently it’s num which is a continuous number. Slightly oddly, as.integer() doesn’t work: it’s still treated as continuous by ggplot, presumably because there are potentially still quite a lot of integers. We could convert to a string with as.character, but we need to start using factors, so let’s do that here. Factors in R were originally a way to save space with character variables in a dataset. In annual_co2 for example, rather than store ‘ALBANIA’ 10 times, for each row, a factor would give ALBANIA a numeric code and store that. The character strings become the levels. [Try z &lt;- as.factor(annual_co2$STATE_NAME) and see what is said for z in the environment pane. rm(z) to tidy up, if you wish.] For this example, we’ll convert YEAR on the fly, with an as.factor in the aes() call. annual_co2 &lt;- annual_co2 %&gt;% mutate(STATE_NAME = str_to_title(STATE_NAME)) ggplot(annual_co2 %&gt;% filter(YEAR %in% c(2010, 2019)), aes(reorder(STATE_NAME, CO2_QTY_TONNES), CO2_QTY_TONNES/1e6, fill = as.factor(YEAR))) + # make discrete geom_col(position = &quot;dodge&quot;) + labs(x = &quot;&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), title = &quot;Aviation Emissions in 2019&quot;, caption = &quot;Source: EUROCONTROL.&quot;, fill = &quot;Year&quot;) + # nicer label for legend coord_flip() Look closely at the graph. What is the sort order? Neither the 2010 nor the 2019 bars are actually in order. We’ve asked reorder to do too much. It seems to have sorted by the total of the 2 years, which is a reasonable thing to do in the circumstances. But I think that’s hard for the user of the graph to interpret, and I’d like the ordering to be by 2019. We can do this ordering with factors. First define a vector that is in the order we want, using arrange() to sort it. The desc() reverses the order. Then define a factor version of the state names, and insist that it’s in this fixed order. factor() is like as.factor() which we used in the previous chunk of code, but allows these extra parameters. # get the state names in the specific order that we want state_order &lt;- annual_co2 %&gt;% filter(YEAR == 2019) %&gt;% # in year 2019 arrange(desc(CO2_QTY_TONNES)) %&gt;% # descending order pull(STATE_NAME) # create an ordered factor with this annual_co2 &lt;- annual_co2 %&gt;% mutate(ordered_states = factor(STATE_NAME, levels = state_order, ordered = TRUE)) ggplot(annual_co2 %&gt;% filter(YEAR %in% c(2010, 2019)), aes(ordered_states, CO2_QTY_TONNES/1e6, fill = as.factor(YEAR))) + # make discrete geom_col(position = &quot;dodge&quot;) + labs(x = &quot;&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), title = &quot;Aviation Emissions&quot;, caption = &quot;Ordering by 2019 emissions. Source: EUROCONTROL.&quot;, fill = &quot;Year&quot;) + # nicer label for legend coord_flip() 5.3.2 Chart facets, more years in the bar chart If you want the reader to compare things, a good rule of thumb is to make sure these things are all in the same graph. We’ve achieved this for comparisons between years and between countries. However, this is a bit of a squeeze, vertically, while there’s lots of empty space. Plus, it doesn’t look like this method would easily cope with a third year, say. ggplot provides a simple way to split charts into ‘facets’, which can sometimes be a way to show variation across a dimension with just a few values (2 or 3 years, say), while aligning the axes in a sensible way. There’s a bit of a twist in the notation: you can’t just mention a variable name (as you can in aes()), you need either to say vars(YEAR) or use a ‘formula’ notation starting with a tilde ~, which involves less typing so that’s what I’ve done here. The _wrap would allow wrapping onto multiple rows, but I just want one row here. ggplot(annual_co2 %&gt;% filter(YEAR %in% c(2010, 2015, 2019)), aes(ordered_states, CO2_QTY_TONNES/1e6)) + geom_col() + facet_wrap(~YEAR, nrow = 1) + labs(x = &quot;&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), title = &quot;Aviation Emissions&quot;, caption = &quot;Source: EUROCONTROL.&quot;) + coord_flip() facet_wrap has given all the x- and y-axes the same matching scale, and not bothered to repeat the y-axis labels. So it’s compact. The graph is not bad for comparing relative sizes of the larger States in a given year, and for seeing how the ranking changes. But it’s not that easy to compare amounts between years. However, we can use facets to split in a different way, if we arbitrarily put the States into two groups. Remember that [ ] is a way to select elements of the vector, in this case the first 19. We could equally have used head(state_order, 19), but the [1:19] is a model that is used more often. We turn off the scale-matching (scales = \"free\"), so really it’s two separate graphs, but with one piece of code. annual_co2 &lt;- annual_co2 %&gt;% mutate(size = if_else(ordered_states %in% state_order[1:19], &quot;Larger Emitters&quot;, &quot;Smaller Emitters&quot;)) ggplot(annual_co2 %&gt;% filter(YEAR %in% c(2010, 2015, 2019)), aes(ordered_states, CO2_QTY_TONNES/1e6, fill = as.factor(YEAR))) + geom_col(position = &quot;dodge&quot;) + facet_wrap(~size, nrow = 1, scales = &quot;free&quot;) + labs(x = &quot;&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), title = &quot;Aviation Emissions&quot;, caption = &quot;Source: EUROCONTROL.&quot;, fill = &quot;Year&quot;) + # nicer label for legend coord_flip() This certainly allows better comparisons of some of the mid-range emitters, and between years. Assuming you’re not after comparison of of Luxembourg and Finland, perhaps this is fit for purpose. There are many different distributions in flight data that have this long tail challenge: with most of the flights in a few airports, or a few countries, or by a few aircraft types. It’s easy enough to switch to a logarithmic scale, but that’s then often hard to read. Facets like this are a reasonable alternative, and adaptable. The results are a little distorted by the length of some of the names, for which a crude solution is in question (2) below. 5.4 What’s gone wrong? If your saved .png file seems very large, check that you specified the units. ggsave defaults to inches. If your axis labels are switched, remember that coord_flip will affect these. 5.5 Exercises 5.5.1 Questions Plot the bar chart of section 5.1 with the longest bars at the bottom. Plot the bar chart of section 5.1 without the 3 near-zero entries. (Hints: View the data. Filter on 2019 and choose a threshold.) Test the statement in the text that aes(..., colour=as.factor(YEAR)) gives an outline to the bars. Use select and ! to remove the size variable again. (Hint: Until you’re sure it works, don’t overwrite your dataset but make a temporary one, eg. start with z &lt;-.) Read the description of geom_bar in the help file. In the first bar chart, switch to using geom_bar instead - to get the same results. (Hint: A minor addition to the aes().) Save the final faceted bar chart to a png file. 5.5.2 Answers Use reorder(state_label, -CO2_QTY_TONNES). Use filter(YEAR == 2019 &amp; CO2_QTY_TONNES &gt; 100000). Did it? z &lt;- annual_co2 %&gt;% select(!size), then if it works, replace z. Replace geom_col with geom_bar and add weight = in front of CO2_QTY_TONNES/1e6, thus switching it from the y parameter to the weight. ggsave(\"graphs/2facet co2.png\", width = 15, height = 10, units = \"cm\") or some other appropriate proportions. Did you save the correct graph? Remember that by default it saves the last one plotted. 5.6 Extended Exercises In chapters 3 to 5 we’ve covered some basics of the R language, seen through the process of building graphs of some emissions data. Before moving on to new concepts, you might like to try some extended exercises. These do not introduce any new functions, but they might use new parameters for functions that you’ve already seen. So a good place to start if you’re stuck is the help file for the functions that you know about. If that doesn’t work, then there are some hints in section 5.6.2, deliberately placed slightly separate from the questions! This book also has a search function, for finding your way back to relevant sections - see the magnifying glass top left. Up and down arrows move from one find to the next. It’s good practice to periodically ‘clean’ your environment, that is, to remove all the data saved in it. This is essential before testing, because you often find that you’re relying on something that has been changed as you tweak and improve the code. Click the broom icon in the environment pane before you start the exercises. 5.6.1 Questions Starting from the final graph in chapter 4.5, move the label to 2015 and set the colour of the label to black. Building on (1) plot the 6 states with fewest flights in 2010, but which had more than 10,000 flights . There’s a bit of overlap, so use a different shape for each one. (Harder) Plot the monthly emissions for France from 2015 onwards as a bar chart. We haven’t done dates yet, so use a character string. 5.6.2 Hints If you’re starting from a ‘clean’ environment, you will need to load the data and re-create top_states. Search in this book for load( and top_states. Mostly a question of filtering. Don’t forget to check the text of your graph when it’s done. The help for geom_point lists which aesthetics are understood, so will help you find the parameter name for changing shape (in this case, it’s the obvious answer). You need to go back to the code that loads the original excel file. You’ll need to mutate to create a new variable combining two strings str_c (or paste), but you need to put this inside an if else to treat 10-12 different to the other months, or the text won’t sort correctly. Remember what we did to the bar charts when there was long text on the x-axis? In the end, it’s an ugly chart but let’s not worry too much about that. 5.6.3 Answers This exercise is really a lesson in assembling the code into one sequence - it gets a bit broken up in the book. You should probably include your library(tidyverse) statement at the top, for really complete code. Notice that we have two colour aesthetics: the default one which is by YEAR and one specifically for the geom_text_repel. The other catch is that, because the colour of the text is constant, it appears outside the aes(), not inside. # load the dataset load(&quot;data/annual_co2.rda&quot;) top_states &lt;- annual_co2 %&gt;% filter(YEAR == 2019) %&gt;% # top in year 2019 slice_max(TF, n = 8) %&gt;% # top 8 pull(STATE_NAME) annual_co2 &lt;- annual_co2 %&gt;% mutate(state_label = if_else(YEAR == 2015, str_to_title(STATE_NAME), &quot;&quot;)) ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% top_states), aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point() + geom_path() + ggrepel::geom_text_repel(aes(label = state_label), colour = &quot;black&quot;) + scale_colour_steps(n.breaks = 8, show.limits = TRUE) + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, title = &quot;Emissions for the busiest 8 states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Busiest&#39; means most flights in 2019.&quot;) This is an exercise in logic in the filter. Did you remember to update the title, footnotes and the legend title? # load the dataset load(&quot;data/annual_co2.rda&quot;) selected_states &lt;- annual_co2 %&gt;% filter(YEAR == 2010 &amp; TF &gt; 10000) %&gt;% # more than 10k flights slice_min(TF, n = 6) %&gt;% # bottom 6 pull(STATE_NAME) annual_co2 &lt;- annual_co2 %&gt;% mutate(state_label = if_else(YEAR == 2015, str_to_title(STATE_NAME), &quot;&quot;)) ggplot(annual_co2 %&gt;% filter(STATE_NAME %in% selected_states), aes(TF/1e6, CO2_QTY_TONNES/1e6, colour = YEAR, group = STATE_NAME)) + geom_point(aes(shape = STATE_NAME)) + geom_path() + ggrepel::geom_text_repel(aes(label = state_label), colour = &quot;black&quot;) + scale_colour_steps(n.breaks = 8, show.limits = TRUE) + labs(x = &quot;Departing Flights (millions)&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), colour = &quot;Year&quot;, shape = &quot;State&quot;, title = &quot;Emissions for the least-busy 6 states&quot;, caption = &quot;Source: EUROCONTROL. &#39;Least-busy&#39; means fewest flights in 2010, but more than 10k flights.&quot;) You could load from Excel and immediately filter. I do it in two steps, because then I can check the correct field names for the filter and mutate using the environment pane. This is an exercise in building up quite complex statements from simple functions. Formatting with line breaks should help a lot. # go back to the excel file aviation_co2 &lt;- readxl::read_excel(&quot;data/CO2_emissions.xlsx&quot;, sheet = &quot;DATA&quot;) # just the data that we need monthly_co2 &lt;- aviation_co2 %&gt;% filter(YEAR &gt;= 2017 &amp; STATE_NAME == &quot;FRANCE&quot;) %&gt;% mutate(date = if_else(MONTH &lt; 10, str_c(YEAR, &quot; &quot;, MONTH), str_c(YEAR, &quot; &quot;, MONTH))) #a pseudo-date # then the basic ggplot(data, aes(x,y)) + geometry... ggplot(monthly_co2, aes(date, CO2_QTY_TONNES/1e6)) + geom_col() + labs(x = &quot;&quot;, y = bquote(~CO[2]~&quot; (million tonnes)&quot;), title = &quot;Aviation Emissions of France&quot;, caption = &quot;Source: EUROCONTROL.&quot;) + coord_flip() "],["groups.html", "6 Looping with groups 6.1 Summarising 6.2 Dates 6.3 Adding variables", " 6 Looping with groups It feels like we have hardly started to investigate the lovely set of data from the SID. It would be natural to compare countries, to produce totals or min and max per country, to compare months, to give just three examples. This calls for a sort of looping: for each country or for each month, do x. In the tidyverse this is done with group_by and a host of supporting functions, as we’ll explore in this chapter. The patterns that we explore in this chapter are some of those that you’ll use most. Often, your main decision is whether you need to summarise (section 6.1) or to change and add variables, mutate in tidyverse-speak (section 6.3), or a bit of both. In this chapter, you’ll be introduced to: lapply, rep, function, pmin, pmax, is.list, unlist, read_xlsx, map, pmap, crossing 6.1 Summarising Summarising radically reduces the number of rows in your dataset. You will also lose variables in the process. We saw an example in section 3.1, where we summarised to get annual totals. For that monthly data, you can think of it row-wise, as “aggregate the rows so that I just have years”, or column-wise, as “get rid of the months”. These are essentially the same, and both are achieved with summarise and related functions. After a summarise all that is left are the variables created by summarise and the grouping variables.7 6.2 Dates 6.3 Adding variables There’s an exception to this rule, with geographic data using package sf that we’ll see in chapter ??.↩︎ "],["splitparse.html", "7 NOTAMS: Splitting and parsing strings 7.1 Splitting a column into many 7.2 List columns", " 7 NOTAMS: Splitting and parsing strings NOTE: This chapter has been written out of sequence, so jumps some steps and may feel a bit of a leap from the (current) previous chapter. Need to add in between chapters on: dates, mutate-across, lapply, named list (extracting from) TBD. This chapter uses the example of NOTAMs (notices to airmen) to explore R functions for turning text into more useable data, by splitting and parsing. The parsing of NOTAMs here is NOT for operational use. It’s for statistical analysis, post operations. NOTAMs are very varied, and we will not handle all the possible ways in which the parsing might fail. We assume that failures can be picked up in later statistical analysis, or will just be outliers that can be ignored as 1 in millions of cases. This code is NOT A SUBSTITUTE FOR READING THE NOTAM BEFORE THE FLIGHT. In this chapter, you’ll be introduced to: separate(), regular expressions, list fields, system.time(), identical(), str_match, str_split(), str_replace(_all)(), str_sub(), unnest_(wider/longer)(), map2(), relocate The sections are ordered not in the order you’d have to work, starting with a full NOTAM text, but in a rough order of simple to more complex. We assume throughout that you have not just a single string, but a dataframe of strings for processing. This is because we want to illustrate this way of working: ‘vectorised’ in the R jargon. You could write a function that takes a single string and parses it. Then call it many times and stack the results together with rbind or bind_rows. But we’re planning to run this millions of times, so that’s millions of function calls. So instead we write in a way that works on the whole dataset at once, and leaves the tidyverse to do that in the most efficient way. (There’s an illustration of the time penalty later in the chapter.) 7.1 Splitting a column into many Within the tidyverse tidyr provides some useful tools for parsing text. In this chapter we see a simpler and a more complex use of tidyr::separate. 7.1.1 Splitting a column at a character In the simplest case, a string is to be separated at a single separator character. In field Q of the NOTAM this is “/”. We just need to tell separate what the new columns are called, as in the following code. library(tidyverse) # a couple of examples, in a dataframe q_fields &lt;- data.frame(q = c(&quot;GLRB/QPLXX/IV/NBO/E /000/999/0620N01206W483&quot;, &quot;LYBA/QKKKK/K /K /K /000/999/4234N02102E999&quot;) ) q_parsed &lt;- q_fields %&gt;% separate(q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), sep = &quot;/&quot;) #pretty view for the book knitr::kable(q_parsed) FIR qgroup IV NBO AEW FL_lo FL_hi geo GLRB QPLXX IV NBO E 000 999 0620N01206W483 LYBA QKKKK K K K 000 999 4234N02102E999 There are some fields here which could be parsed further (qgroup and geo), but we leave that. I suspect that the ‘K’ is a missing value, which we could translate into an R NA value, but we don’t go that far here. We will assume that all the q-fields are properly formed, of these 8 fields. A more robust piece of code would check this is true. This is for analysis, not operations - you’re getting the message I think. Inspecting the raw strings (always essential!) shows that there are some blank spaces. So in our result we have some fields with trailing spaces. We could use stringr::str_trim on the results to get rid of trailing spaces, and indeed, that would probably be a good idea anyway. But we can use this to illustrate the next level in use of separate, which we do in the next section. 7.1.2 Splitting a column at a regular expression The sep= is a ‘regular expression’. Entire books have been written about regular expressions, so my advice would be to find a page you like, or download a cheatsheet of which there are many. This is the one I use, being R-focused. A regular expression is a search or matching pattern. We want to split the q-field at not just \\ but any number of spaces followed by a \\. That will remove the trailing spaces, because they’ll be treated as part of the separator. Our string has some ‘newlines’ in it \\n as well as spaces, and in other applications perhaps tabs are used. So we use a \\\\s code, rather than just a space character \" \". \\\\s stands for all of these types of ‘space’, and some others [check in the cheatsheet]. In regular expression terms this is \\\\s*/, which you can read as zero-or-more spaces followed by a back slash. We can show that this gives the same result as using trim. # what you&#39;d get using trim q_parsed_trim &lt;- q_fields %&gt;% separate(q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), sep = &quot;/&quot;) %&gt;% mutate(across(.fns = str_trim)) #what you get with the search pattern q_parsed_regex &lt;- q_fields %&gt;% separate(q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), # separate on any number of spaces followed by a backslash sep = &quot;\\\\s*/&quot;) # are they the same? identical(q_parsed_trim, q_parsed_regex) ## [1] TRUE They are identical. Which do you prefer? The second involves one less line of code, but you might feel the need to add a comment, as I’ve done here, because the code is more obscure. So it’s the same number of lines to read. That’s a qualitative judgement. If you wanted to run this thousands of times, maybe you would run timing tests. Something like this. q_many &lt;- data.frame(q = rep(q_fields$q, 100000)) # what you&#39;d get using trim system.time( q_parsed_trim &lt;- q_many %&gt;% separate(q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), sep = &quot;/&quot;) %&gt;% mutate(across(.fns = str_trim)) ) ## user system elapsed ## 4.16 3.00 7.17 #what you get with the search pattern system.time( q_parsed_regex &lt;- q_many %&gt;% separate(q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), # separate on any number of spaces followed by a backslash sep = &quot;\\\\s*/&quot;) ) ## user system elapsed ## 3.760 0.231 3.994 # and for fun, what does calling the function many times look like? # just do 1/100th of the rows system.time( z &lt;- lapply(q_many$q[1:1000], function(x) data.frame(q=x) %&gt;% separate(q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), sep = &quot;\\\\s*/&quot;)) ) ## user system elapsed ## 1.225 0.074 1.301 # are they the same? identical(q_parsed_trim, q_parsed_regex) ## [1] TRUE On my machine, the first takes about 40% longer in elapsed time, and twice the system time. So having fewer lines of code is more efficient in this case. In a light-heared way, we also show a timing test for calling the function each time. Yes, the code that isn’t carefully designed for speed, but for a dataset 100 times smaller already takes longer than either of the other two. Letting R do its vectorised thing is indeed better! 7.1.3 Splitting a column at an either-or Taking this one step further, we can also work on the NOTAM header. This has NOTAM in it, which isn’t that useful information, so we can ditch in, but it has this in the form ‘NOTAMC’, ‘NOTAMR’, ‘NOTAMN’ depending on whether this is cancelling, replacing or a new NOTAM, respectively. The structure of the field varies between these cases. As always, there are several ways to approach this. We could first remove the ‘NOTAM’ then split at spaces. But that’s two steps. So instead, we split at either ’ NOTAM’ or ’ ’ , in one go. In regular expressions, either or is given by |, just as in R syntax for ‘or’. And to be on the safe side, we use the more general code for a ‘space’, though a new line is unlikely in this position. header_fields &lt;- data.frame(header = c( &quot;A0001/13 NOTAMN &quot;, &quot;A0001/13 NOTAMR A0032/12&quot;, &quot;W0809/13 NOTAMC W0808/13&quot; )) header_parsed &lt;- header_fields %&gt;% separate(header, c(&quot;notam&quot;, &quot;type&quot;, &quot;replaces&quot;), sep = &quot;\\\\sNOTAM|\\\\s&quot;) #pretty view for the book knitr::kable(header_parsed) notam type replaces A0001/13 N A0001/13 R A0032/12 W0809/13 C W0808/13 This works neatly, partly because there’s a trailing space at the end of the first one. [In the code, delete the space and re-run. What happens? How does the header-parsed dataframe change?] In bulk use, you could either live with the Expected 3 pieces.... errors, or add an extra space. Probably this is a sign that we should look for a better way to split, but this will always be tricky when the number of parts varies from one to the next, as it does here. 7.1.4 Exercises 7.1.4.1 Questions If \\\\s matches a space, what matches everything that is not a space? How did the header_parsed dataframe change when you removed the space? Look up the format of q_parsed$geo on the internet. Split it into 3 parts of fixed length, not based on a separator, but appropriately to the meaning. 7.1.4.2 Answers \\\\S, from the cheat sheet. There’s also a more complex version of the same thing [^[:space]]. One value changed from empty string, \"\", to NA. The help file explains that sep can also be positions. Try separate(geo, c(\"Lat\",\"Long\",\"Radius\"), c(5, 11)). 7.2 List columns In the last section we saw various ways to split one column into several. Here we use another approach. Conceptually, it’s harder, but it keep the parts ‘similar’ for longer, so we can modify them further en masse rather than one column at a time. As usual this is not the only way this could be done, but it’s a good example of the use of list columns. List columns? Up to now, you might have been thinking of tidy datasets like a table in a spreadsheet. Not a bad analogy, but R has more tricks up its sleeve. One particularly powerful one is that any column can contain, not just single values, but lists. There’s a whole package in tidyverse called purrr (yes, 3 ’r’s) to help handle such things. List columns are great for modelling. You can have rows with just 3 cells, say, one a country name, the next the whole historic time series, the next the forecast. Extremely compact. The application here is simpler. The pattern we’ll introduce here has 4 steps: Split a complex text column into a list (one column) Flip that list to generate extra rows in the dataset Manipulate those rows, in groups Pivot to generate 1 row per text, again, and multiple columns. Why might this be easier than approaches using separate? Because we want to manipulate what would be the column names after separate, it’s easier to do if these are instead text fields. 7.2.1 Splitting a field into a list We load a set of NOTAM texts from Github. These are based on real ones, but some have been manipulated to create parsing challenges that have been seen in larger sets. The problems are in the later ones, so we will use short samples to begin with: start with a small slice is a good principle to follow anyway. (see TBD handling large datasets) Sites like Eurocontrol’s EAD Basic provide views onto ready-parsed NOTAM data. So this chapter is relevant either (a) to learn string manipulation with regular expressions or (b) if you happen to receive bulk data for analysis, or both. When adding rows, as we will, we need an index that lets us identify all of the rows that go with one piece of text. We generate a simple integer index for this. The initial processing is to remove the opening and closing bracket from the whole NOTAM. This is just for sake of tidiness, though once we split the field the “()” are no longer a matched pair, so best to do it now. [How does the regular expression ^\\\\(|\\\\)$ achieve this?] NOTAMs are in sections, with headers “Q)”, “A)”, “B)” to “G)”. So, in a first attempt (which works in very simple cases only), we want to split where we find ‘section’ headers “Q)”, “A)” etc. But we don’t want to throw these away, we want to split and keep the separator. The regular expression is \\\\s+[QABCDEFG]\\\\)\\\\s to match any of the allowable headers in a NOTAM. You’ve seen most of these parts, the only new one is [QABCDEFG], which says any one of these characters (case-sensitive). The extra bit is (?= ) which says: and leave behind the value corresponding to the part after the =, after splitting on its position. We choose to forget any leading spaces, so the \\\\s+ is before this bracketed term. # get the NOTAM texts and add an ID notam_url &lt;- &quot;https://github.com/david6marsh/flights_in_R/raw/main/data/NOTAMSample.csv&quot; notam_texts &lt;- read.csv(notam_url) %&gt;% mutate(index = row_number()) notams &lt;- notam_texts %&gt;% slice(1:2) %&gt;% #just a few to start with mutate( # remove start and end brackets full = stringr::str_replace_all(NOTAM, &quot;^\\\\(|\\\\)$&quot;, &quot;&quot;), # split out the phrases based on Q) A) etc phrases = stringr::str_split(full, &quot;\\\\s+(?=[QABCDEFG]\\\\)\\\\s)&quot;)) # quick look at one list. notams$phrases[1] ## [[1]] ## [1] &quot;A0011/10 NOTAMN&quot; ## [2] &quot;Q) PAZA/QMRXX/IV/NBO/A /000/999/6034N15115W005&quot; ## [3] &quot;A) PAEN&quot; ## [4] &quot;B) 1001011230&quot; ## [5] &quot;C) 1001031230 EST&quot; ## [6] &quot;E) RWY 01L/19R PATCHY THIN ICE SANDED&quot; That’s what a single list element looks like when you print it to the log. Have a look in the Environment pane to see what a list column looks like. It should like this, depending on how wide your window is! The dataframe has 2 observations. So phrases is shown as a “List of 2”. It has to be the same length as the number of rows in the dataframe. Each element in phrases is also a list, so it’s a list of lists, but each element can be of any length. [Check by inspection, or run lapply(notams$phrases, length).] 7.2.2 More realistic splitting In real NOTAMs, “A)” can be a header as we saw above, but it can also be a bullet for a bulleted list within field “E)”. After trying a number of approaches to parsing this, I’ve opted here to split the NOTAM into 2 parts at the first “E)” and handle them separately. In fact it allows me to illustrate some more features of list fields. If there were only one “E)” we could just use separate to do this. But “E)” can also appear as a bullet, so we take a multi-step approach: split at “E)” using str_split because we can insist on only have 2 parts, ie splitting at the first occurrence; add names to the resulting list column, using setNames; then unnest_wider, ie into columns. If this isn’t an obvious approach, don’t worry, because it wasn’t obvious to me either. It took trial and error, and a bit of googling to find this combination. The error was finding the hard way that unnest_wider won’t let you provide names for the new columns (as we saw above that separate does), it insists on finding them in the data. [You can see the ‘errors’ by commenting out the line with map in it.] The googling was how to provide names for unnest_wider. As usual, someone has already struggled with this, and an answer was out there. notams &lt;- notam_texts %&gt;% slice(1:7) %&gt;% #all but the last one mutate( # remove start and end brackets full = stringr::str_replace_all(NOTAM, &quot;^\\\\(|\\\\)$&quot;, &quot;&quot;), # split into two parts split_at_E = stringr::str_split(full, &quot;\\\\s(?=E\\\\)\\\\s)&quot;, n=2), split_at_E = map(split_at_E, setNames, c(&quot;part1&quot;, &quot;part2&quot;)) ) %&gt;% unnest_wider(split_at_E) We can now split out the fields from the first part as in our first attempt, splitting at \\\\s+(?=[QABCD]\\\\)\\\\s). The second part is still tricky, because “F)” and “G)” can be fields or bullets in a bulleted list. The two fields are rare, and describe the vertical extent of the NOTAM. We find the ‘real’ fields by looking for something that resembles a vertical extent. Inspection of many cases shows we have values such as: 1200FT AMSL (above mean sea level), or that could be in metres ‘M’ or above ground level (AGL) or surface SFC. There might or might not be a space between. [What would the regular expression look like?] FL310 (flight level) or just SFC or GND (ground) for the bottom (field F) or UNL (unlimited) in either field. Put all of these together and we get an expression like ffg in the next code chunk; we could have found slightly different expressions for F and G, but this single one works for both, so it’s good enough for us here. Use the cheatsheet for terms that may be new (eg \\\\d). It can be easier to read if you look for () pairs, starting with ones close together, and for the | symbol. [Which expression allows for a possible but not necessary space? Find each the listed items above, separated by |.] Having split, and generated two list fields, we need to recombine them. Normally you’d join list with c(), eg c(list(1), list(7,5)). Here we do that too, but within the mutate function have to write it in a longer form, using map2 (TBD - explain reasons?). Then, finally, we get to step (2) of our pattern (remember that?) and un-nest into rows. ffg &lt;- &quot;(?=[FG]\\\\)\\\\s*((\\\\s*\\\\d+(FT|M)\\\\s*(AGL|AMSL|SFC))|(FL\\\\d+)|SFC|GND|UNL))&quot; # before the unnest, how many rows do we have nrow(notams) ## [1] 7 notams &lt;- notams %&gt;% #split these - note force the first to be a list column mutate(part1 = str_split(part1, &quot;\\\\s+(?=[QABCD]\\\\)\\\\s)&quot;), part2 = str_split(part2, ffg), both = purrr::map2(part1, part2, c)) %&gt;% select(!c(part1, part2)) %&gt;% unnest_longer(both) # show that we&#39;ve added quite a few rows nrow(notams) ## [1] 45 [Check visually that in NOTAM 4, the fields F and G have been pulled out but that in NOTAM 6 bullets F and G are left within field E.] If we had used separate in place of str_split then we would generate missing fields NA, and potentially (if there were a D field but no C, say) map data into the wrong field. [Exercise - try with separate] 7.2.3 Manipulate fields in rows Step 3 is to generate what will become the column names. After all those regular expressions, we’ll stick to some basic extraction of parts of strings for this. We want the first element to be called header. So we use groups as we saw earlier (TBD), and row_number to get the first item. Then converting to columns is done with pivot_wider. [Why not unnest? Because we not dealing with list columns now, but ‘ordinary’ ones.] The order in which this happens is not quite perfect, so we relocate it, even if it will make no difference to the analysis, it will help us when scrolling back and forth in the data viewer. notams &lt;- notams %&gt;% group_by(index) %&gt;% # sort out the names mutate(id = if_else(row_number()==1, &quot;header&quot;, str_sub(both, 1, 1)), # having taken field id, remove the id from the text both = if_else(id == &quot;header&quot;, both, str_sub(both, 4, nchar(both)))) %&gt;% # then convert to columns pivot_wider(names_from = id, values_from = both, names_prefix = &quot;field_&quot;) %&gt;% ungroup() %&gt;% # no longer need this index and NOTAM is a near-duplicate of full select(!c(index, NOTAM)) %&gt;% # put D in its place relocate(field_D, .before = field_E) That’s steps 3 and 4 done; we’ve finished extracting the fields from the raw text. We can move on to parsing the fields, to pull out information from then. In fact, we already did this in section 7.1.1. You could also convert text to dates. Pulling forward the code from those earlier sections we get something like this. notams &lt;- notams %&gt;% mutate( from_date = as.POSIXct(field_B, format = &quot;%y%m%d%H%M&quot;, tz = &quot;UTC&quot;), #for C we will get errors but that&#39;s OK to_date = as.POSIXct(field_C, format = &quot;%y%m%d%H%M&quot;, tz = &quot;UTC&quot;)) %&gt;% # split some fields # this warns on generating NAs, which is what we want separate(field_header, c(&quot;notam&quot;, &quot;type&quot;, &quot;replaces&quot;), sep = &quot;( NOTAM| )&quot;) %&gt;% separate(field_Q, c(&quot;FIR&quot;, &quot;qgroup&quot;, &quot;IV&quot;, &quot;NBO&quot;, &quot;AEW&quot;, &quot;FL_lo&quot;, &quot;FL_hi&quot;, &quot;geo&quot;), sep = &quot;/&quot;) %&gt;% # put them in a helpful order relocate(from_date, .after = field_B) %&gt;% relocate(to_date, .after = field_C) %&gt;% relocate(full, .after = last_col()) That’s as far as we need to go. The code generates some warnings of ‘missing pieces’, but otherwise works on these examples. The exercises explore a bit further, including how this ‘basic’ set of code might fail for some NOTAMs. 7.2.4 Exercises 7.2.4.1 Questions How does the regular expression ^\\\\(|\\\\)$ achieve what we need? What would str_replace have done differently to str_replace_all? stringr::str_replace_all would also accept a vector of regular expressions. Predict what using c(\"^\\\\(\",\"\\\\)$\") would deliver. Then try it out. [Hint: there are 2 rows.] In \\\\s+(?=[QABCDEFG]\\\\)\\\\s), which closing bracket goes with the opening bracket? Create a dataframe with a named list column, for which each entry is a named list. [Hint: I find this surprisingly fiddly. Create a 2-row, 1 field dataframe. Then add the named list column.] How can you pull out a single value from this? Use str_match to pull the A field directly from the NOTAM field in notam_texts. Try two approaches, one assuming that the string ends in “B)” and another based only on the likely content of the field. Create a function that takes a dataframe like ‘notam_texts’ and returns the result of the whole process. What happens if you run it on the last NOTAM in the file and why? In words, how might you solve this? In words, how might you solve the ‘missing pieces’ warnings? Where would our code struggle if a NOTAM has repeated fields? (I’ve seen this occur: two different (!) versions of fields F and G)? 7.2.4.2 Answers The cheat sheet will tell you: ^ matches at the start of the string, $ at the end. ( and ) are reserved tokens in regular expressions, so you need to ‘escape’ them to use them as literal characters. | we saw earlier, means ‘or’. It just changes the first occurrence. Compare str_replace(q_fields$q, \"/\", \"-$-\") and str_replace_all(q_fields$q, \"/\", \"-$-\"). The usual R behaviour is to cycle through the values. So it alternates, using the first for the odd rows (ie the first one) and the second for the even rows. So the first loses the starting ( and the second loses just the ending ). The first closing bracket is preceded by \\\\ so it is treated as a literal character, not part of the code. By elimination, then, it has to be the second one. For example z &lt;- data.frame(id = 1:2) z$named &lt;- list(a=c(x=1, y=2), b=c(z=4)) With that example, you can use z$named$a[\"y\"], for example. Amongst many possible solutions, these work. How would you select A if it contains multiple 3 or 4 letter strings separated by spaces? ans &lt;- notam_texts %&gt;% mutate(v1 = str_match(NOTAM, &quot;A\\\\)\\\\s+[^(B\\\\))]+&quot;), v2 = str_match(NOTAM, &quot;A\\\\)\\\\s+[[:upper:]]{4}&quot;), # multiply one field, for testing notam_plus = str_replace(NOTAM, &quot;RPXX&quot;, &quot;RPXX EGLL&quot;), vMult = str_match(notam_plus, &quot;A\\\\)(\\\\s+[[:upper:]]{4})+&quot;)) (Essentially a cut-paste exercise, so result not provided here.) The final NOTAM has a missing “E)” header for the E field, so the splitting approach fails. Two possible solutions: filter to remove, if “E)” not found would perhaps be ok as one outlier amongst thousands; or detect and insert “E)” after C or D to allow the code to work properly. Perhaps there’s a ‘quiet’ option for the function ;-). Otherwise, since the problem is whether or not there’s text to say which NOTAM is being replaced, you could test for length and separate in two different ways: if_else(header is long, separate with 3 column names, separate with 2 column names). A hard one, but this leads pivot_wider() to create a list column rather than an ordinary one. A nice robust response, but possibly not what you want. In practice, I used the additional parameters values_from = both, values_fn = gdata::first to handle this and arbitrarily pick the first of the duplicate entries. This might not suit your need. "],["references.html", "References", " References "]]
